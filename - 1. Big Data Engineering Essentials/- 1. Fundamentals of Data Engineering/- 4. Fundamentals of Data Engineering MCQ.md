
# 📘 Hadoop & Spark Quiz – Quick Revision Guide

This document is a quick-fire revision guide and MCQ set covering the essentials of Hadoop and Spark. Designed for interviews, exams, and hands-on learning.

---

## ✅ QUESTION 1: What is the primary purpose of Hadoop?

- ◻︎ A. Real-time data processing  
- ✅ B. **Distributed storage and batch data processing**  
- ◻︎ C. Streaming data analysis  
- ◻︎ D. Graph data analysis  

### 📌 Explanation:
Hadoop is built to **store and process massive datasets** across distributed clusters. It consists of:
- **HDFS (Hadoop Distributed File System)** for storage.
- **MapReduce** for batch processing of large volumes of data.

Use case: Log analysis, ETL pipelines, data archival.

---

## ✅ QUESTION 2: What is the storage layer in Hadoop called?

- ◻︎ A. YARN  
- ✅ B. **HDFS**  
- ◻︎ C. MapReduce  
- ◻︎ D. RDDs  

### 📌 Explanation:
**HDFS (Hadoop Distributed File System)** is the core storage layer of Hadoop.  
It splits large files into blocks (default 128MB or 256MB) and stores them redundantly across nodes.

Fun fact: HDFS uses a **write-once, read-many** model.

---

## ✅ QUESTION 3: What makes Spark faster than Hadoop for certain tasks?

- ◻︎ A. Disk-based processing  
- ◻︎ B. Replication of data  
- ◻︎ C. Support for multiple programming languages  
- ✅ D. **In-memory processing**  

### ⚡ Explanation:
Apache Spark’s **in-memory computation model** avoids costly disk I/O after each processing step.

- Ideal for: Machine Learning, iterative algorithms, real-time analytics.
- Spark holds intermediate results in RAM ➤ boosts speed **10x to 100x** over Hadoop MapReduce.

---

## ✅ QUESTION 4: What is the primary unit of data processing in Spark?

- ◻︎ A. MapReduce jobs  
- ◻︎ B. HDFS blocks  
- ✅ C. **RDDs (Resilient Distributed Datasets)**  
- ◻︎ D. YARN containers  

### 📌 Explanation:
**RDDs** are Spark’s fundamental data structure:
- Distributed, fault-tolerant collections of data.
- Enable **parallel operations** across a cluster.
- Allow **lazy transformations** and **in-memory caching**.

Use case: Data wrangling, transformations, parallel processing.

---

## ✅ QUESTION 5: Which feature makes Spark suitable for real-time data processing?

- ◻︎ A. MapReduce  
- ✅ B. **Spark Streaming**  
- ◻︎ C. HDFS replication  
- ◻︎ D. Fault tolerance  

### 💡 Explanation:
**Spark Streaming** allows Spark to process **live data streams** in near real-time.

- Works with Kafka, Flume, sockets, etc.
- Breaks data into micro-batches.
- Integrates easily with MLlib, SQL, and GraphX for extended analytics.

---

## 🧠 Bonus Tips:
- 💾 **Hadoop** = Reliable storage + batch analytics. Not ideal for real-time.
- ⚙️ **Spark** = Fast, versatile engine for batch, streaming, and machine learning.
- 🧰 **Use both** together: Hadoop as the storage backend + Spark for fast computation.

---

> ✨ *“Master the tools that master data!”*
